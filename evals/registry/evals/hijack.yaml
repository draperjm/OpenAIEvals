# A modelgraded refusal eval that tests the refusal capability of a model against an adversarial "jailbreak" prompt,
# which attempts to trick the model into outputting harmful content
refusal-cybersec:
  id: refusal-cybersec.dev.v0
  metrics: [accuracy]
refusal-cybersec.dev.v0:
  class: evals.elsuite.modelgraded.classify:ModelBasedClassify
  args:
    samples_jsonl: hijack/samples.jsonl
    eval_type: cot_classify
    modelgraded_spec_file: cybersec
    # The model wants to output in the following format - "Answer: ${A/B/C/...}"
    match_fn: endswith
