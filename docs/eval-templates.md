# 既存の評価用テンプレート

評価を使用する中で、我々は多くの異なるベンチマークに対応するいくつかの「テンプレート」を発見しました。これらのテンプレートを `evals/elsuite` に実装することで、新しい評価の開発を簡略化することができます。これらのテンプレートを使えば、多くの評価は実装のためのコーディングが不要になると考えています！その代わりに、既存のテンプレートの中から1つを選び、データセットとパラメータを指定するだけでよいのです。

## 基本的な評価テンプレート

選択式の問題の解答や、答えが単純な質問など、望ましいモデルのレスポンスのバリエーションが少ないケースでは、以下のようなテンプレートが有効だと考えています。

モデル補完 `a` と正解の参照リスト `B` に対して、以下の評価を実装する。
- [`basic/match.py:Match`](../evals/elsuite/basic/match.py): `any([b.startswith(a) for b in B])`
- [`basic/fuzzy_match.py:FuzzyMatch`](../evals/elsuite/basic/fuzzy_match.py): `any([(a in b or b in a) for b in B])`

どの評価テンプレートを使用するかは、ユースケースによって異なります。プロンプト（または模範解答）をどのように、あるいはどう微調整し評価テンプレートを選択するかを決定するのに役立つので、モデルのcompletionを検査することを常にお勧めします。 学術的なベンチマークはこのような基本的な評価の型に当てはまることが多く、学術的な評価の端から端までの例をJupyterノートブックとして `examples` フォルダに実装しています。

場合によっては、[カスタム評価ロジック](custom-eval.md)の方がニーズに合っていることがあります。例えば、[機械翻訳](../evals/elsuite/translate.py) [eval example](../examples/lafand-mt.ipynb) では、evalに使用したいユニークで明確に定義された指標が存在します。評価のロジックをカスタマイズするか、基本的な評価テンプレートを使用するか、次に説明するモデルによる評価を使用するかを決める際には、最善の判断を下す必要があります。

## モデルによる評価テンプレート

自由記述の質問への解答など、モデルのレスポンスに大きなばらつきがあるケースでは、モデル自身を採点することが、自動評価のための有効な戦略であることが分かっています。一般的に、評価モデルと評価されるモデルは同じである必要はありませんが、ここでは説明を簡単にするために同じであると仮定します。

[`modelgraded/classify.py:ModelBasedClassify`](../evals/elsuite/modelgraded/classify.py) は、モデルグレードの評価テンプレートのメインロジックを実装しています。要するに、モデルのプロンプトを取得し、それを評価プロンプトでラップし、モデルのプロンプトを評価プロンプトで取得し、それを解析して目的の指標に変換します。重要なのは、評価プロンプトはモデルが簡単にパースできるような方法で解答する必要があることです。例えば、選択式や単純な「はい/いいえ」で解答します。以下にモデルによる評価例を示しますが、まずこの評価テンプレートのパラメータを指定します。

### モデルによる評価のパラメータ

これらのパラメータがコード内でどのように使用されているかは、[`classify.py:ModelBasedClassify`](../eval/elsuite/modelgraded/classify.py) クラスを参照してください。

- `prompt`: このプロンプトは、元のプロンプトに対するモデルのプロンプトとcompletionを取り込み、他の情報と一緒に、簡単にパースできるような評価を提供するようにモデルを誘導します。 中括弧で囲まれた部分（すなわち `{key}` ）は、データの `input_outputs` または追加の `args` （下記参照）から入力されます。
- `input_outputs`:どの入力を使ってどのような補完を生成するかを指定するマッピングです。多くの評価では、入力と補完のペアは1つだけですが、2つの補完を互いに比較する場合などには、もっと多く存在することがあります。
- `choice_strings`: 評価プロンプトが与えられたときに、モデル補完に含まれると予想される選択肢。例: `"ABCDE"` または `["Yes", "No", "Unsure"]`. モデルが返すその他の選択肢は `"__invalid__"` にパースされます。
- `choice_scores` （オプション）:それぞれの選択肢のスコアへのマッピングで、指標としてログに記録さ れます。例：「はい」（「いいえ」）のレスポンスがモデルの元の完成度が良かった（悪かった）ことを示す場合、この選択肢に1（0）のスコアを割り当てることができます。
- `eval_type` （オプション）: 評価プロンプトに対するモデルのレスポンスをどのようにフォーマットするか。現在、サポートされているオプションは以下の通りです。
  - `"cot_classify"` ("chain-of-thought then classify"、すなわち、理由→解答)は、レスポンスのパース可能な部分(すなわち、選択肢を含む部分)が、コンプリートの端から端までの間にあることを期待します。一般的に最も正確なモデルグレード評価を提供するため、デフォルトとしてこの方法を推奨します。
  - `"classify_cot"` (解答→理由）は、モデルレスポンスに最初に選択肢が含まれることを想定しています。
  - `"classify"` は、モデルのレスポンスに選択肢だけが含まれることを想定しています。

  `eval_type`を指定する方法は2つあります。推奨される方法は `evals/registry/evals` の YAML ファイルに記述する方法です。この方法で指定すると、モデルを期待通りのフォーマットに誘導するために `prompt` に命令が自動的に付加されます（[コード](./eval/elsuite/modelgraded/classify.py)の `ANSWER_PROMPTS` を参照してください）。あるいは、`eval_type` を `evals/registry/modelgraded` の YAML で指定することもできますが、`prompt` に直接適切な命令を記述する必要があります。
- args` (オプション)。指定すると、複数の評価コールが行われ、それぞれのコールに対して異なる引数のセットで評価プロンプトが変更されます。
- `completion_sample_templates` (オプション): 指定すると、モデルの出力（`multicomp_n > 1`の場合は出力）が補完の中でどのようにフォーマットされるかを決定する。

### モデルによる評価例

モデルグレードの評価を行うには、`evals/registry/modelgraded`にYAMLファイルを作成し、上記の引数に値を指定します。モデルグレードの評価を作成するためのプロセスを説明するために、いくつかの例を提供しましたがこれらの例は、多くの評価ですぐに使えるような一般的なものであると私たちは考えています。

[`fact.yaml`](../evals/registry/modelgraded/fact.yaml): 補完 `a` と模範解答 `b` が与えられると、事実に基づく整合性のある 評価 を返す:
- `"A"` if `a` $\subseteq$ `b`, すなわち、提出された解答は専門家の解答のサブセットであり、それと完全に一致する。
- `"B"` if `a` $\supseteq$ `b`, すなわち、提出された解答は専門家の解答のスーパーセットであり、それと完全に一致する。
- `"C"` if `a` $=$ `b`, すなわち、提出された解答は専門家の解答と同じ内容をすべて含んでいる。
- `"D"` if `a` $\neq$ `b`, すなわち、提示された解答と専門家の解答の間に不一致がある場合です。
- `"E"` if `a` $\approx$ `b`, 解答は異なるが、その違いは事実に基づいているかどうかという観点からは問題ではない。

[`closedqa.yaml`](../evals/registry/modelgraded/closedqa.yaml): 質問に答える評価モデルで、質問と質問に答えるために必要な情報を含むプロンプトが与えられたとき、モデルの解答が正しいかどうかをチェックします。
- 関連する、すなわち指定されたプロンプトを提供する情報から抽出されたものである。
- 簡潔、すなわち不必要な詳細や情報を含んでいない。
- 正しい、すなわち抽出された情報を使って正しい結論を出す。

この評価モデルは、より一般的には評価プロンプトを指定し、与えられた基準をチェックし、上記の要求事項を1つずつ入力させる「基準チェック型」評価として実装されていることにご注意ください。私たちは興味深い基準を詳しく記した「ルーブリック」を指定し、同じプロンプトとイエス/ノーの選択肢に従うことで他の多くの評価モデルが実装できると考えています。

[`battle.yaml`](../evals/registry/modelgraded/battle.yaml): 2つの異なるプロンプトに対する2つのモデルのcompletionを比較する、直接対決の評価です。ここでは `choice_scores` を使って、最初のcompletionが2番目のcompletionよりも優れていると判断される頻度をログに記録しています。

また、より具体的なモデル能力（ユーモアなど）をテストする例もありますが、他の評価への汎用性は低いと思われます。しかし、これらの例は評価プロンプトの書き方やモデルによる評価の設定方法の違いについて説明するのに役立ちます。モデルによる評価をビルドするための詳細な手順については、[このセクション] (build-eval.md#for-model-graded-evals-a-step-by-step-workflow) を参照してください。

