# 評価のビルド

この文書では、データセットとevalクラスの選択から成るevalを構築するためのエンドツーエンドのプロセスを説明します。examplesフォルダには、いくつかの学術的なevalを構築するための手順を示すJupyterノートブックが含まれており、全体的なプロセスを説明するのに役立ちます。

このプロセスのステップは、データセットの構築、新しい評価のデータセットへの登録、そして評価の実行です。重要なことは、私たちはあなたが[既存の評価テンプレート](eval-templates.md)をそのまま使用していると仮定しています（もし違う場合は、[カスタム評価の構築例](custom-eval.md)を参照してください）。もし、あなたがあなたの評価を公開したい場合、我々は何を興味深い評価と考えるかの基準を以下に示しています。

以下のカテゴリーで評価を募集しています。

- 過剰な拒否反応
- 安全性
- システムメッセージの操作可能性
- 現実世界における幻覚
- 数学／論理／物理的な推論
- 現実のユースケース（この機能が商品でどのように使用されるかをPRで記述してください。）
- その他の基礎的能力

このカテゴリーから外れていても、多様な事例となる評価があれば、ぜひ投稿してください。

## データをフォーマットする

実装したい評価が決まったら、サンプルを正しいJSON行（JSONL）フォーマットに変換する必要があります。JSONLファイルは、1行に1つのユニークなJSONオブジェクトを持つ、ただのJSONファイルです。

JSONLの評価ファイルの例を[registry/data/README.md](../eval/registry/data/README.md)で紹介しています。

各JSONオブジェクトは、評価の1つのデータポイントを表します。JSONオブジェクトに必要なキーは、評価のテンプレートに依存します。すべてのテンプレートで `"input"` キーが必要とされ、プロンプトは理想的には [chat format](https://platform.openai.com/docs/guides/chat/introduction) で指定されているものとします (文字列もサポートされています)。チャット以外のモデルを評価する場合でも、チャットフォーマットを推奨します。チャットモデルと非チャットモデルの両方を評価する場合、チャットフォーマットのプロンプトと生の文字列プロンプトの間での変換を行います（変換ロジックは [こちら](../eval/prompt/base.py) を参照してください）。

基本的な評価である `Match`、`Includes`、`FuzzyMatch` において、その他のキーとして必要なものは `"ideal"` で、これは正しい基準解答を指定する文字列 (または文字列のリスト) です。モデルによる評価では必要なキーは評価によって異なりますが、（オプションの） `args` でカバーできない `{key}` を `prompt` で指定することで決定されます。

[CoQA](https://stanfordnlp.github.io/coqa/)データセットの小さなサブセットを様々なEvalテンプレートに実装し、データがどのようにフォーマットされる必要があるかを説明しました。基本的な評価テンプレートである `Match` に適したデータの例として [`coqa/match.jsonl`](../evals/registry/data/coqa/match.jsonl) を、`fact` と `closedqa` モデルによる評価に適したデータとして [`coqa/samples.jsonl`](../evals/registry/data/coqa/samples.jsonl) を参考にしています。
この2つのモデルによる評価では異なるキーを想定していますが、両方の評価をサポートするためにデータにキーのスーパーセットを含めることができることに注意してください。

データセットファイルがローカルマシン上にある場合は、`jsonl`ファイルを `evals/registry/evals/data/<eval_name>/samples.jsonl` に置いてください。Cloud Object Storageにある場合は、主要なクラウドのパススタイルのURLをサポートします（個人的な利用に限定し、クラウドURLでのPRは受け付けません）。

## 評価の登録について

elsuiteのレジストリフォーマットを用いて、`eval/registry/eval/<eval_name>.yaml`にファイルを追加して、評価を登録します。例えば、`Match`の評価の場合、次のようになります:
```
<eval_name>:
  id: <eval_name>.dev.v0
  metrics: [accuracy]

<eval_name>.dev.v0:
  class: evals.elsuite.basic.match:Match
  args:
    samples_jsonl: <eval_name>/samples.jsonl
```

例えば `test_match/samples.jsonl` が提供されたファイルであれば、データは `evals/registry/data/test_match/samples.jsonl` にあることが期待されます。

評価の命名規則は、`<eval_name>.<split>.<version>`という形式になっています。
- `<eval_name>`は評価の名前で、スコアが同程度の評価をグループ化するために使用します。
- 同じ`<base_eval>`の下にある評価をさらにグループ化するために使用される`<split>`はデータスプリットです。例えば、"val"、"test"、"dev" などをテスト用に使用します。
- `<version>` は評価のバージョンで、任意の説明テキストを使用することができます（ただし、". "を含まないことが望ましいです）。

一般的に同じモデルに対して同じ評価を実行すると常に同じような結果が得られ、他の人がそれを再現できるようになります。従って、評価を変更した場合はバージョンを上げる必要があります。

## 評価の実行

これでCLIからデータを使って、好きなモデルで評価を実行することができるようになりました。
```
oaieval gpt-3.5-turbo <eval_name>
```
おめでとうございます！これで評価（eval）をビルドすることができました！結果に自信が持てるようになるまで、繰り返し練習してください。

## モデルによる評価のため：ステップバイステップのワークフロー

我々は、`fact`、`closedqa`、`battle`といった既存のモデルによる評価が、多くのユースケースに適合することを期待しています。しかし、他のユースケースでは、例えば異なる評価プロンプトのような、より多くのカスタマイズのメリットを享受できるかもしれません。このような場合は、もう少し作業が必要になりますが、通常はコーディングの必要はありません。

1. 既存のモデルによる評価を利用できない場合は、`evals/registry/modelgraded`に新しいYAMLを作成しこのモデルによる評価の[パラメータ] (eval-templates.md#parameters-for-model-graded-evals) を指定してください。例として [`humor.yaml`](../evals/registry/modelgraded/humor.yaml) を参照してください。
    - 新しい YAML を作成する場合でも、出発点として既存の YAML をコピーするのが一番簡単だと気づくかもしれないことに注目してください。例えば、モデルの完成度をルールに基づいてチェックするモデルによる評価では `closedqa.yaml` をコピーして `args` を編集するだけでよいでしょう。
2. 次に、上記のようにデータセットを作成し、評価を登録します。例えば、[`joke_fruits_labeled.jsonl`](../eval/registry/data/test_metaeval/joke_fruits_labeled.jsonl)や[`joke-fruits`](../eval/registry/eval/test-modelgraded.yaml)などを見てください。
    - なお、評価を登録する際には、ステップ1ではなく、このステップの時点で`eval_type`を指定することが推奨されます。
3. 例えば`oaleval gpt-3.5-turbo joke-fruits`のように、評価を実行します。
4. (推奨) モデルによる評価用のメタ評価を追加する! モデルによる評価には、主に `prompt` や `eval_type` など、調整するためのノブがいくつかあります。モデルによる評価を高品質にするために、モデルによる評価には "choice labels "をつけることを推奨します。choice labelsとは、モデルがどの評価を選択すべきかを示す、人間が提供するラベルのことです。例として、(これらのジョークが実際に面白いということにして) [`joke_fruits_labeled.jsonl`](../evals/registry/data/test_metaeval/joke_fruits_labeled.) の `"choice"` キーを見てください。 jsonl)、これは `joke-fruits` 評価 では使用されませんが、そのすぐ下の [`joke-fruits-meta`](../evals/registry/evals/test-modelgraded.yaml) メタ評価 では使用されます . メタ評価の実行後、例えば `oaieval gpt-3.5-turbo joke-fruits-meta` とすると、レポートには `metascore/` という精度が出力されますが、これは良いモデルによる評価では "1.0" に近い精度になるはずです。

## 評価を投稿する基準

重要: コードを提供する場合、`black`, `isort`, `autoflake` が実行されていることを確認するために、コミットおよびプッシュする前に `pip install pre-commit; pre-commit install` を実行していることを確認してください。

私たちは、今後モデルを改善するために多様で興味深い評価セットを収集・整理することに関心を持っています。ここでは私たちが考える良い評価の基準をいくつか紹介します。
- [ ] 評価はテーマに沿った一貫性がある必要があります。同じ用途、主題領域、障害モードなどを中心とした複数のプロンプトを見たいと思っています。
- [ ] 評価は挑戦的である必要があります。もしGPT-4やGPT-3.5-Turboがすべてのプロンプトでうまくいった場合、それはあまり興味深くありません。もちろん、モデルの制限や制約に応じて評価が可能である必要もあります。しばしば、良い目安は人間（可能なら専門家）がプロンプトでうまくやれるかどうかです。
- [ ] 評価の方向性は明確であるべきです。データには、何が正しい行動なのかを示す適切なシグナルが含まれていなければなりません。これは、例えば、高品質の模範解答や、解答を評価するための包括的な基準を意味します。
- [ ] 評価は慎重に行われる必要があります。提出する前に、良好なパフォーマンスのためにプロンプトを工夫したか、最良の評価テンプレートを使用しているか、結果の正確性を確認するためにスポットチェックを行ったかなど、検討する必要があります。

Evalを公開する準備ができたら、PRを提出してください。OpenAIチームは、喜んでそのEvalに目を通します。 PRメッセージに事前に入力されているテンプレートの全ての項目が記入されていることを確認してください。PRを提出しても、OpenAIが最終的にマージすることを保証するものではないことにご注意ください。どのEvalをフォローアップするか検討する際に、私たちは独自のチェックを実行し、最善の判断を下します。
