{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NKMeHicMAByU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773,
     "referenced_widgets": [
      "8b643cb65b7744de8007337e13be2e5d",
      "62d7ccf32bce4ffc8d3eab0200491148",
      "3338738d2b524a999248e8f03e1800ad",
      "29ea450849e14000a7b53667213d0272",
      "d85408eeb32a44d0828305cc383c6333",
      "5fe34d8fe55b4989887f00e0e2eb38cc",
      "70f254c3bd00492982f290ef7b679b44",
      "9adf6aa633ea4d8c8ee7785664a8505c",
      "6f0a42538b7e4962bf9c90681571dfd4",
      "2b213951066e4dedbed7f31c3cbbd86a",
      "bf2ae41b9f7242c79cb7d6e82c2342e1"
     ]
    },
    "executionInfo": {
     "elapsed": 2872057,
     "status": "error",
     "timestamp": 1681970439943,
     "user": {
      "displayName": "김정석",
      "userId": "11353705873413895385"
     },
     "user_tz": -540
    },
    "id": "NKMeHicMAByU",
    "outputId": "c80a4f9c-99b5-41da-8905-ef66e044514a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import mimetypes\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import time\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import dropbox\n",
    "import numpy as np\n",
    "import requests\n",
    "from IPython.core.display_functions import display\n",
    "from ipywidgets import widgets\n",
    "from requests import HTTPError\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def load_config():\n",
    "    config_path = input(\"Enter the path of the config file: \")\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    return config\n",
    "\n",
    "\n",
    "def download_file(url, local_path):\n",
    "    urlretrieve(url, local_path)\n",
    "\n",
    "\n",
    "def download_file_from_dropbox(dropbox_link, local_path):\n",
    "    dbx = dropbox.Dropbox(\"<your_dropbox_access_token>\")\n",
    "    with open(local_path, \"wb\") as f:\n",
    "        metadata, res = dbx.files_download(dropbox_link)\n",
    "        f.write(res.content)\n",
    "\n",
    "\n",
    "def extract_file(source, target_dir, source_type=\"local\"):\n",
    "    if source_type == \"url\":\n",
    "        response = requests.get(source)\n",
    "        file_bytes = io.BytesIO(response.content)\n",
    "        mime_type, encoding = mimetypes.guess_type(source)\n",
    "    elif source_type == \"dropbox\":\n",
    "        local_path = \"/tmp/temp_file\"\n",
    "        download_file_from_dropbox(source, local_path)\n",
    "        with open(local_path, \"rb\") as f:\n",
    "            file_bytes = io.BytesIO(f.read())\n",
    "        mime_type, encoding = mimetypes.guess_type(source)\n",
    "    else:  # local\n",
    "        with open(source, \"rb\") as f:\n",
    "            file_bytes = io.BytesIO(f.read())\n",
    "        mime_type, encoding = mimetypes.guess_type(source)\n",
    "\n",
    "    supported_formats = {\n",
    "        \"application/x-gzip\": \".tar.gz\",\n",
    "        \"application/x-tar\": \".tar\",\n",
    "        \"application/zip\": \".zip\",\n",
    "        \"application/x-7z-compressed\": \".7z\",\n",
    "        \"application/x-rar-compressed\": \".rar\",\n",
    "    }\n",
    "\n",
    "    if mime_type in supported_formats:\n",
    "        ext = supported_formats[mime_type]\n",
    "        local_path = \"/tmp/temp_file\" + ext\n",
    "        with open(local_path, \"wb\") as f:\n",
    "            f.write(file_bytes.getvalue())\n",
    "        shutil.unpack_archive(local_path, target_dir)\n",
    "        os.remove(local_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Please use .tar, .tar.gz, .zip, .7z, or .rar\")\n",
    "\n",
    "\n",
    "def read_imdb_data(data_dir, categories=['pos', 'neg'], max_size=None):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if any(category in root for category in categories):\n",
    "                sentiment = 'positive' if 'pos' in root else 'negative'\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    data.append(f.read())\n",
    "                    labels.append(sentiment)\n",
    "\n",
    "            if max_size is not None and len(data) >= max_size:\n",
    "                break\n",
    "\n",
    "        if max_size is not None and len(data) >= max_size:\n",
    "            break\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def get_sentiment(text, config, retries=3, initial_backoff=1):\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {config[\"api_key\"]}'\n",
    "    }\n",
    "    data = {\n",
    "        'model': 'gpt-3.5-turbo',\n",
    "        'messages': [\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': 'You are an AI language model trained to perform sentiment analysis. Determine if the sentiment of the following text is positive, negative, or neutral:'\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': text\n",
    "            }\n",
    "        ],\n",
    "        'temperature': 0.7,\n",
    "        'max_tokens': 32,\n",
    "        'n': 1,\n",
    "    }\n",
    "    url = 'https://api.openai.com/v1/chat/completions'\n",
    "\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "            response.raise_for_status()\n",
    "            break\n",
    "        except HTTPError as e:\n",
    "            if response.status_code == 429:\n",
    "                backoff_time = initial_backoff * (2 ** i)\n",
    "                print(f\"Rate limit exceeded. Waiting for {backoff_time} seconds before retrying.\")\n",
    "                time.sleep(backoff_time)\n",
    "            else:\n",
    "                raise e\n",
    "    else:\n",
    "        raise HTTPError(\"Too many retries. Please check your rate limits.\", response=response)\n",
    "\n",
    "    result = json.loads(response.content)\n",
    "    sentiment = result['choices'][0]['message']['content'].strip().lower()\n",
    "    sentiment_match = re.search(r'(positive|negative|neutral)', sentiment)\n",
    "    if sentiment_match:\n",
    "        return sentiment_match.group(0)\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "\n",
    "def analyze_sentiments(texts, labels, config, dataset_size, stop_button):\n",
    "    confusion_matrix = {\n",
    "        'positive': {'positive': 0, 'negative': 0, 'neutral': 0, 'mixed': 0},\n",
    "        'negative': {'positive': 0, 'negative': 0, 'neutral': 0, 'mixed': 0},\n",
    "    }\n",
    "\n",
    "    for i, (text, true_label) in enumerate(tqdm(zip(texts, labels), total=dataset_size, desc=\"Analyzing sentiments\")):\n",
    "        if i >= dataset_size or stop_button.value:\n",
    "            break\n",
    "        predicted_label = get_sentiment(text, config)\n",
    "        if predicted_label not in confusion_matrix[true_label]:\n",
    "            predicted_label = 'positive' if true_label == 'negative' else 'negative'\n",
    "\n",
    "        confusion_matrix[true_label][predicted_label] += 1\n",
    "\n",
    "    return confusion_matrix\n",
    "\n",
    "\n",
    "def evaluate_performance(confusion_matrix, epsilon=1e-9):\n",
    "    metrics = {}\n",
    "    for true_label, row in confusion_matrix.items():\n",
    "        tp = row[true_label]\n",
    "        fp = sum([v for k, v in row.items() if k != true_label])\n",
    "        fn = sum([confusion_matrix[k][true_label] for k in confusion_matrix.keys() if k != true_label])\n",
    "\n",
    "        precision = tp / (tp + fp + epsilon)\n",
    "        recall = tp / (tp + fn + epsilon)\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall + epsilon)\n",
    "\n",
    "        metrics[true_label] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1_score,\n",
    "        }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def main():\n",
    "    config = load_config()\n",
    "    data_dir = input(\"Enter the path to the IMDB dataset: \")\n",
    "    initial_dataset_size = int(input(\"Enter the initial dataset size: \"))\n",
    "    texts, labels = read_imdb_data(data_dir, max_size=initial_dataset_size)  # Change max_size to initial_dataset_size\n",
    "    texts = np.array(texts)\n",
    "    labels = np.array(labels)\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    overall_metrics = {\n",
    "        'positive': {'precision': 0, 'recall': 0, 'f1_score': 0},\n",
    "        'negative': {'precision': 0, 'recall': 0, 'f1_score': 0},\n",
    "    }\n",
    "\n",
    "    stop_button = widgets.ToggleButton(value=False, description='Stop Analysis', button_style='danger')\n",
    "    change_size_button = widgets.Button(description='Change Dataset Size')\n",
    "    display(stop_button)\n",
    "    display(change_size_button)\n",
    "\n",
    "    def change_dataset_size(b):\n",
    "        nonlocal texts, labels, initial_dataset_size\n",
    "        dataset_size = int(input(\"Enter the new dataset size: \"))\n",
    "        texts, labels = read_imdb_data(data_dir, max_size=dataset_size)\n",
    "        texts = np.array(texts)\n",
    "        labels = np.array(labels)\n",
    "        initial_dataset_size = dataset_size\n",
    "\n",
    "    change_size_button.on_click(change_dataset_size)\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(texts, labels), start=1):\n",
    "        print(f\"Running fold {i} of 5\")\n",
    "        test_texts, test_labels = texts[test_index], labels[test_index]\n",
    "        dataset_size = initial_dataset_size\n",
    "        confusion_matrix = analyze_sentiments(test_texts, test_labels, config, dataset_size, stop_button)\n",
    "\n",
    "        metrics = evaluate_performance(confusion_matrix)\n",
    "\n",
    "        print(f\"Fold {i} metrics:\")\n",
    "        print(json.dumps(metrics, indent=2))\n",
    "\n",
    "        for label, label_metrics in metrics.items():\n",
    "            for metric, value in label_metrics.items():\n",
    "                overall_metrics[label][metric] += value\n",
    "\n",
    "    for label, label_metrics in overall_metrics.items():\n",
    "        for metric, value in label_metrics.items():\n",
    "            overall_metrics[label][metric] = value / 5\n",
    "\n",
    "    print(\"Overall metrics:\")\n",
    "    print(json.dumps(overall_metrics, indent=2))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YfNqTD6f-bNE",
   "metadata": {
    "cellView": "form",
    "id": "YfNqTD6f-bNE",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import tarfile\n",
    "from collections import defaultdict\n",
    "from google.colab import files\n",
    "\n",
    "import openai\n",
    "import requests\n",
    "\n",
    "\n",
    "def upload_files():\n",
    "    uploaded = files.upload()\n",
    "    tf = tarfile.open(fileobj=io.BytesIO(uploaded['aclImdb_v1.tar.gz']), mode='r:gz')\n",
    "    tf.extractall()\n",
    "\n",
    "\n",
    "def load_config(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    return config\n",
    "\n",
    "\n",
    "def read_imdb_data(data_dir):\n",
    "    data = {}\n",
    "    labels = {}\n",
    "\n",
    "    for dataset_type in ['train', 'test']:\n",
    "        data[dataset_type] = []\n",
    "        labels[dataset_type] = []\n",
    "\n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            sentiment_dir = os.path.join(data_dir, dataset_type, sentiment)\n",
    "            data[dataset_type].extend(\n",
    "                [open(os.path.join(sentiment_dir, f), 'r', encoding='utf-8').read() for f in os.listdir(sentiment_dir)])\n",
    "            label = 'positive' if sentiment == 'pos' else 'negative'\n",
    "            labels[dataset_type].extend([label] * len(os.listdir(sentiment_dir)))\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def get_sentiment(text, config):\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {config[\"api_key\"]}'\n",
    "    }\n",
    "    data = {\n",
    "        'model': 'gpt-3.5-turbo',\n",
    "        'messages': [\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': 'You are an AI language model trained to perform sentiment analysis. Identify the sentiment of the following text:'\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': text\n",
    "            }\n",
    "        ],\n",
    "        'temperature': 0.7,\n",
    "        'max_tokens': 32,\n",
    "        'n': 1,\n",
    "    }\n",
    "    url = 'https://api.openai.com/v1/chat/completions'\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    if response.ok:\n",
    "        result = json.loads(response.content)\n",
    "        sentiment = result['choices'][0]['message']['content'].strip().lower()\n",
    "        sentiment_match = re.search(r'(positive|negative|neutral|mixed)', sentiment)\n",
    "        if sentiment_match:\n",
    "            return sentiment_match.group(0)\n",
    "        else:\n",
    "            return 'unknown'\n",
    "    else:\n",
    "        response.raise_for_status()\n",
    "\n",
    "\n",
    "def analyze_sentiments(texts, labels, config):\n",
    "    confusion_matrix = {\n",
    "        'positive': {'positive': 0, 'negative': 0, 'neutral': 0, 'mixed': 0},\n",
    "        'negative': {'positive': 0, 'negative': 0, 'neutral': 0, 'mixed': 0},\n",
    "    }\n",
    "\n",
    "    for text, true_label in zip(texts, labels):\n",
    "        predicted_label = get_sentiment(text, config)\n",
    "\n",
    "        if predicted_label not in confusion_matrix[true_label]:\n",
    "            predicted_label = 'positive' if true_label == 'negative' else 'negative'\n",
    "\n",
    "        confusion_matrix[true_label][predicted_label] += 1\n",
    "\n",
    "    return confusion_matrix\n",
    "\n",
    "\n",
    "def evaluate_performance(confusion_matrix):\n",
    "    metrics = {}\n",
    "    for true_label, row in confusion_matrix.items():\n",
    "        tp = row[true_label]\n",
    "        fp = sum([v for k, v in row.items() if k != true_label])\n",
    "        fn = sum([confusion_matrix[k][true_label] for k in confusion_matrix.keys() if k != true_label])\n",
    "\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "        metrics[true_label] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1_score,\n",
    "        }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def calculate_accuracy(confusion_matrix):\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for true_label, row in confusion_matrix.items():\n",
    "        total_correct += row[true_label]\n",
    "        total_samples += sum(row.values())\n",
    "\n",
    "    return total_correct / total_samples\n",
    "\n",
    "\n",
    "def main():\n",
    "    # upload_files()\n",
    "    config = load_config('/content/config.json')\n",
    "    openai.api_key = config['api_key']\n",
    "\n",
    "    data_dir = '/content/aclImdb'\n",
    "    data, labels = read_imdb_data(data_dir)\n",
    "\n",
    "    sample_size = 10\n",
    "    texts = random.sample(data['test'], sample_size)\n",
    "    test_labels = [labels['test'][data['test'].index(t)] for t in texts]\n",
    "\n",
    "    confusion_matrix = analyze_sentiments(texts, test_labels, config)\n",
    "\n",
    "    metrics = evaluate_performance(confusion_matrix)\n",
    "    accuracy = calculate_accuracy(confusion_matrix)\n",
    "\n",
    "    print(f\"\\nAccuracy: {accuracy:.4f}\\n\")\n",
    "    print(\"Sentiment analysis results:\")\n",
    "    for sentiment, metric_values in metrics.items():\n",
    "        print(f\"{sentiment.capitalize()}:\")\n",
    "        for metric, value in metric_values.items():\n",
    "            print(f\"  {metric.capitalize()}: {value:.4f}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aQvz5r0S5TB1",
   "metadata": {
    "cellView": "form",
    "id": "aQvz5r0S5TB1",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import tarfile\n",
    "from collections import defaultdict\n",
    "from google.colab import files\n",
    "\n",
    "import openai\n",
    "import requests\n",
    "\n",
    "\n",
    "def upload_files():\n",
    "    uploaded = files.upload()\n",
    "    tf = tarfile.open(fileobj=io.BytesIO(uploaded['aclImdb_v1.tar.gz']), mode='r:gz')\n",
    "    tf.extractall()\n",
    "\n",
    "\n",
    "def load_config(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    return config\n",
    "\n",
    "\n",
    "def read_imdb_data(data_dir):\n",
    "    data = {}\n",
    "    labels = {}\n",
    "\n",
    "    for dataset_type in ['train', 'test']:\n",
    "        data[dataset_type] = []\n",
    "        labels[dataset_type] = []\n",
    "\n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            sentiment_dir = os.path.join(data_dir, dataset_type, sentiment)\n",
    "            data[dataset_type].extend(\n",
    "                [open(os.path.join(sentiment_dir, f), 'r', encoding='utf-8').read() for f in os.listdir(sentiment_dir)])\n",
    "            label = 'positive' if sentiment == 'pos' else 'negative'\n",
    "            labels[dataset_type].extend([label] * len(os.listdir(sentiment_dir)))\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def get_sentiment(text, config):\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {config[\"api_key\"]}'\n",
    "    }\n",
    "    data = {\n",
    "        'model': 'gpt-3.5-turbo',\n",
    "        'messages': [\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': 'You are an AI language model trained to perform sentiment analysis. Identify the sentiment of the following text:'\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': text\n",
    "            }\n",
    "        ],\n",
    "        'temperature': 0.7,\n",
    "        'max_tokens': 32,\n",
    "        'n': 1,\n",
    "    }\n",
    "    url = 'https://api.openai.com/v1/chat/completions'\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    if response.ok:\n",
    "        result = json.loads(response.content)\n",
    "        sentiment = result['choices'][0]['message']['content'].strip().lower()\n",
    "        sentiment_match = re.search(r'(positive|negative|neutral|mixed)', sentiment)\n",
    "        if sentiment_match:\n",
    "            return sentiment_match.group(0)\n",
    "        else:\n",
    "            return 'unknown'\n",
    "    else:\n",
    "        response.raise_for_status()\n",
    "\n",
    "\n",
    "def analyze_sentiments(texts, labels, config):\n",
    "    confusion_matrix = {\n",
    "        'positive': {'positive': 0, 'negative': 0, 'neutral': 0, 'mixed': 0},\n",
    "        'negative': {'positive': 0, 'negative': 0, 'neutral': 0, 'mixed': 0},\n",
    "    }\n",
    "\n",
    "    for text, true_label in zip(texts, labels):\n",
    "        predicted_label = get_sentiment(text, config)\n",
    "\n",
    "        if predicted_label not in confusion_matrix[true_label]:\n",
    "            predicted_label = 'positive' if true_label == 'negative' else 'negative'\n",
    "\n",
    "        confusion_matrix[true_label][predicted_label] += 1\n",
    "\n",
    "    return confusion_matrix\n",
    "\n",
    "\n",
    "def evaluate_performance(confusion_matrix):\n",
    "    metrics = {}\n",
    "    for true_label, row in confusion_matrix.items():\n",
    "        tp = row[true_label]\n",
    "        fp = sum([v for k, v in row.items() if k != true_label])\n",
    "        fn = sum([confusion_matrix[k][true_label] for k in confusion_matrix.keys() if k != true_label])\n",
    "\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "        metrics[true_label] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1_score,\n",
    "        }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def main():\n",
    "    # upload_files()\n",
    "    config = load_config('/content/config.json')\n",
    "    openai.api_key = config['api_key']\n",
    "\n",
    "    data_dir = '/content/aclImdb'\n",
    "    data, labels = read_imdb_data(data_dir)\n",
    "\n",
    "    n = 10\n",
    "    texts = random.sample(data['test'], n)\n",
    "    test_labels = [labels['test'][data['test'].index(t)] for t in texts]\n",
    "\n",
    "    confusion_matrix = analyze_sentiments(texts, test_labels, config)\n",
    "\n",
    "    metrics = evaluate_performance(confusion_matrix)\n",
    "\n",
    "    print(\"\\nSentiment analysis results:\")\n",
    "    for sentiment, metric_values in metrics.items():\n",
    "        print(f\"{sentiment.capitalize()}:\")\n",
    "        for metric, value in metric_values.items():\n",
    "            print(f\"  {metric.capitalize()}: {value:.4f}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vnH8VVsPzx9O",
   "metadata": {
    "cellView": "form",
    "id": "vnH8VVsPzx9O",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "import openai\n",
    "import requests\n",
    "from google.colab import files\n",
    "\n",
    "\n",
    "def upload_files():\n",
    "    uploaded = files.upload()\n",
    "    tf = tarfile.open(fileobj=io.BytesIO(uploaded['aclImdb_v1.tar.gz']), mode='r:gz')\n",
    "    tf.extractall()\n",
    "\n",
    "\n",
    "def load_config(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    return config\n",
    "\n",
    "\n",
    "def read_imdb_data(data_dir):\n",
    "    data = {}\n",
    "    labels = {}\n",
    "\n",
    "    for dataset_type in ['train', 'test']:\n",
    "        data[dataset_type] = []\n",
    "        labels[dataset_type] = []\n",
    "\n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            sentiment_dir = os.path.join(data_dir, dataset_type, sentiment)\n",
    "            data[dataset_type].extend(\n",
    "                [open(os.path.join(sentiment_dir, f), 'r', encoding='utf-8').read() for f in os.listdir(sentiment_dir)])\n",
    "            labels[dataset_type].extend([sentiment] * len(os.listdir(sentiment_dir)))\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def get_sentiment(text, config):\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {config[\"api_key\"]}'\n",
    "    }\n",
    "    data = {\n",
    "        'model': 'gpt-3.5-turbo',\n",
    "        'messages': [\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': 'You are an AI language model trained to perform sentiment analysis. Identify the sentiment of the following text:'\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': text\n",
    "            }\n",
    "        ],\n",
    "        'temperature': 0.7,\n",
    "        'max_tokens': 32,\n",
    "        'n': 1,\n",
    "    }\n",
    "    url = 'https://api.openai.com/v1/chat/completions'\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    if response.ok:\n",
    "        result = json.loads(response.content)\n",
    "        sentiment = result['choices'][0]['message']['content'].strip().lower()\n",
    "        sentiment_match = re.search(r'(positive|negative|neutral|mixed)', sentiment)\n",
    "        if sentiment_match:\n",
    "            return sentiment_match.group(0)\n",
    "        else:\n",
    "            return 'unknown'\n",
    "    else:\n",
    "        response.raise_for_status()\n",
    "\n",
    "\n",
    "def analyze_sentiments(texts, config):\n",
    "    stats = {}\n",
    "    total = len(texts)\n",
    "    for text in texts:\n",
    "        sentiment = get_sentiment(text, config)\n",
    "        stats[sentiment] = stats.get(sentiment, 0) + 1\n",
    "\n",
    "    stats = {key: (value / total) * 100 for key, value in stats.items()}\n",
    "    return stats\n",
    "\n",
    "\n",
    "def main():\n",
    "    # upload_files()\n",
    "    config = load_config('/content/config.json')\n",
    "    openai.api_key = config['api_key']\n",
    "\n",
    "    data_dir = '/content/aclImdb'\n",
    "    data, labels = read_imdb_data(data_dir)\n",
    "\n",
    "    n = 10\n",
    "    texts = random.sample(data['test'], n)\n",
    "\n",
    "    stats = analyze_sentiments(texts, config)\n",
    "\n",
    "    print(\"\\nSentiment analysis results:\")\n",
    "    for sentiment, percentage in stats.items():\n",
    "        print(f\"{sentiment.capitalize()}: {percentage:.2f}%\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_IJ8oZSDyZTe",
   "metadata": {
    "cellView": "form",
    "id": "_IJ8oZSDyZTe",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "from google.colab import files\n",
    "import tarfile\n",
    "import io\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import openai\n",
    "\n",
    "def upload_files():\n",
    "    uploaded = files.upload()\n",
    "    tf = tarfile.open(fileobj=io.BytesIO(uploaded['aclImdb_v1.tar.gz']), mode='r:gz')\n",
    "    tf.extractall()\n",
    "\n",
    "def load_config(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    return config\n",
    "\n",
    "def read_imdb_data(data_dir):\n",
    "    data = {}\n",
    "    labels = {}\n",
    "    \n",
    "    for dataset_type in ['train', 'test']:\n",
    "        data[dataset_type] = []\n",
    "        labels[dataset_type] = []\n",
    "\n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            sentiment_dir = os.path.join(data_dir, dataset_type, sentiment)\n",
    "            data[dataset_type].extend([open(os.path.join(sentiment_dir, f), 'r', encoding='utf-8').read() for f in os.listdir(sentiment_dir)])\n",
    "            labels[dataset_type].extend([sentiment] * len(os.listdir(sentiment_dir)))\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "def get_sentiment(text, config):\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {config[\"api_key\"]}'\n",
    "    }\n",
    "    data = {\n",
    "        'model': 'gpt-3.5-turbo',\n",
    "        'messages': [\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': 'You are an AI language model trained to perform sentiment analysis. Identify the sentiment of the following text:'\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': text\n",
    "            }\n",
    "        ],\n",
    "        'temperature': 0.7,\n",
    "        'max_tokens': 32,\n",
    "        'n': 1,\n",
    "    }\n",
    "    url = 'https://api.openai.com/v1/chat/completions'\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    if response.ok:\n",
    "        result = json.loads(response.content)\n",
    "        sentiment = result['choices'][0]['message']['content'].strip().lower()\n",
    "        return sentiment\n",
    "    else:\n",
    "        response.raise_for_status()\n",
    "\n",
    "def analyze_sentiments(texts, config):\n",
    "    stats = {}\n",
    "    total = len(texts)\n",
    "    for text in texts:\n",
    "        sentiment = get_sentiment(text, config)\n",
    "        stats[sentiment] = stats.get(sentiment, 0) + 1\n",
    "    \n",
    "    stats = {key: (value / total) * 100 for key, value in stats.items()}\n",
    "    return stats\n",
    "\n",
    "def main():\n",
    "    # upload_files()\n",
    "    config = load_config('/content/config.json')\n",
    "    openai.api_key = config['api_key']\n",
    "\n",
    "    data_dir = '/content/aclImdb'\n",
    "    data, labels = read_imdb_data(data_dir)\n",
    "\n",
    "    N = 10\n",
    "    texts = data['test'][:N]\n",
    "\n",
    "    stats = analyze_sentiments(texts, config)\n",
    "\n",
    "    print(\"\\nSentiment analysis results:\")\n",
    "    for sentiment, percentage in stats.items():\n",
    "        print(f\"{sentiment.capitalize()}: {percentage}%\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i_7NRp4Qw9uY",
   "metadata": {
    "cellView": "form",
    "id": "i_7NRp4Qw9uY",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "from google.colab import files\n",
    "import tarfile\n",
    "import io\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import openai\n",
    "\n",
    "def upload_files():\n",
    "    uploaded = files.upload()\n",
    "    tf = tarfile.open(fileobj=io.BytesIO(uploaded['aclImdb_v1.tar.gz']), mode='r:gz')\n",
    "    tf.extractall()\n",
    "\n",
    "def load_config(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    return config\n",
    "\n",
    "def read_imdb_data(data_dir):\n",
    "    \"\"\"Reads the IMDB dataset and returns the data and labels.\"\"\"\n",
    "    data = {}\n",
    "    labels = {}\n",
    "    \n",
    "    for dataset_type in ['train', 'test']:\n",
    "        data[dataset_type] = []\n",
    "        labels[dataset_type] = []\n",
    "\n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            sentiment_dir = os.path.join(data_dir, dataset_type, sentiment)\n",
    "            data[dataset_type].extend([open(os.path.join(sentiment_dir, f), 'r', encoding='utf-8').read() for f in os.listdir(sentiment_dir)])\n",
    "            labels[dataset_type].extend([sentiment] * len(os.listdir(sentiment_dir)))\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "def get_sentiment(text, config):\n",
    "    \"\"\"Returns the sentiment of the given text using the GPT-3.5-turbo model.\"\"\"\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {config[\"api_key\"]}'\n",
    "    }\n",
    "    data = {\n",
    "        'model': 'gpt-3.5-turbo',\n",
    "        'messages': [\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': 'You are an AI language model trained to perform sentiment analysis. Identify the sentiment of the following text:'\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': text\n",
    "            }\n",
    "        ],\n",
    "        'temperature': 0,\n",
    "        'max_tokens': 32,\n",
    "        'n': 1,\n",
    "    }\n",
    "    url = 'https://api.openai.com/v1/chat/completions'\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    if response.ok:\n",
    "        result = json.loads(response.content)\n",
    "        sentiment = result['choices'][0]['message']['content'].strip()\n",
    "\n",
    "        if 'The sentiment of the text is' in sentiment:\n",
    "            sentiment_value = sentiment.split('The sentiment of the text is ')[1].split('.')[0]\n",
    "            return sentiment_value.lower()\n",
    "        else:\n",
    "            return 'neutral'\n",
    "    else:\n",
    "        response.raise_for_status()\n",
    "\n",
    "def analyze_sentiments(texts, config):\n",
    "    \"\"\"Analyzes the sentiments of the given texts and returns the sentiment statistics.\"\"\"\n",
    "    stats = {'positive': 0, 'negative': 0, 'neutral': 0}\n",
    "    total = len(texts)\n",
    "    for text in texts:\n",
    "        sentiment = get_sentiment(text, config)\n",
    "        stats[sentiment] = stats.get(sentiment, 0) + 1\n",
    "    \n",
    "    stats = {key: (value / total) * 100 for key, value in stats.items()}\n",
    "    return stats\n",
    "\n",
    "def main():\n",
    "    # upload_files()\n",
    "    config = load_config('/content/config.json')\n",
    "    openai.api_key = config['api_key']\n",
    "\n",
    "    data_dir = '/content/aclImdb'\n",
    "    data, labels = read_imdb_data(data_dir)\n",
    "\n",
    "    N = 10\n",
    "    texts = data['test'][:N]\n",
    "\n",
    "    stats = analyze_sentiments(texts, config)\n",
    "\n",
    "    print(\"\\nSentiment analysis results:\")\n",
    "    print(f\"Positive: {stats['positive']}%\")\n",
    "    print(f\"Negative: {stats['negative']}%\")\n",
    "    print(f\"Neutral: {stats['neutral']}%\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1uZwmBfhlXBg",
   "metadata": {
    "cellView": "form",
    "id": "1uZwmBfhlXBg",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "from google.colab import files\n",
    "import tarfile\n",
    "import io\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import openai\n",
    "\n",
    "uploaded = files.upload()\n",
    "tf = tarfile.open(fileobj=io.BytesIO(uploaded['aclImdb_v1.tar.gz']), mode='r:gz')\n",
    "tf.extractall()\n",
    "\n",
    "# Load the JSON file\n",
    "with open('/content/config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Access the sensitive text examples\n",
    "openai.api_key = config['api_key']\n",
    "\n",
    "# Function to read IMDB dataset\n",
    "def read_imdb_data(data_dir):\n",
    "    data = {}\n",
    "    labels = {}\n",
    "    \n",
    "    for dataset_type in ['train', 'test']:\n",
    "        data[dataset_type] = []\n",
    "        labels[dataset_type] = []\n",
    "\n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            sentiment_dir = os.path.join(data_dir, dataset_type, sentiment)\n",
    "            for filename in os.listdir(sentiment_dir):\n",
    "                with open(os.path.join(sentiment_dir, filename), 'r', encoding='utf-8') as f:\n",
    "                    data[dataset_type].append(f.read())\n",
    "                    labels[dataset_type].append(sentiment)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "# Load the dataset\n",
    "data_dir = '/content/aclImdb'\n",
    "data, labels = read_imdb_data(data_dir)\n",
    "\n",
    "# Use the first N reviews from the test dataset for analysis\n",
    "N = 10\n",
    "texts = data['test'][:N]\n",
    "\n",
    "# Define the get_sentiment function\n",
    "def get_sentiment(text):\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {config[\"api_key\"]}'\n",
    "    }\n",
    "    data = {\n",
    "        'model': 'gpt-3.5-turbo',\n",
    "        'messages': [\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': 'You are an AI language model trained to perform sentiment analysis. Identify the sentiment of the following text:'\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': text\n",
    "            }\n",
    "        ],\n",
    "        'temperature': 0,\n",
    "        'max_tokens': 32,\n",
    "        'n': 1,\n",
    "    }\n",
    "    url = 'https://api.openai.com/v1/chat/completions'\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    if response.ok:\n",
    "        result = json.loads(response.content)\n",
    "        sentiment = result['choices'][0]['message']['content'].strip()\n",
    "\n",
    "        if 'The sentiment of the text is' in sentiment:\n",
    "            sentiment_value = sentiment.split('The sentiment of the text is ')[1].split('.')[0]\n",
    "            return sentiment_value.lower()\n",
    "        else:\n",
    "            return 'neutral'\n",
    "    else:\n",
    "        response.raise_for_status()\n",
    "\n",
    "# Define the analyze_sentiments function\n",
    "def analyze_sentiments(texts):\n",
    "    stats = {'positive': 0, 'negative': 0, 'neutral': 0}\n",
    "    total = len(texts)\n",
    "    for text in texts:\n",
    "        sentiment = get_sentiment(text)\n",
    "        if sentiment in stats:\n",
    "            stats[sentiment] += 1\n",
    "    \n",
    "    stats = {key: (value / total) * 100 for key, value in stats.items()}\n",
    "    return stats\n",
    "\n",
    "# Run the sentiment analysis\n",
    "stats = analyze_sentiments(texts)\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nSentiment analysis results:\")\n",
    "print(f\"Positive: {stats['positive']}%\")\n",
    "print(f\"Negative: {stats['negative']}%\")\n",
    "print(f\"Neutral: {stats['neutral']}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_9xs6nJBiIKH",
   "metadata": {
    "cellView": "form",
    "id": "_9xs6nJBiIKH",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "import requests\n",
    "import json\n",
    "import openai\n",
    "\n",
    "# Load the JSON file\n",
    "with open('/content/config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Access the sensitive text examples\n",
    "openai.api_key = config['api_key']\n",
    "\n",
    "def get_sentiment(text):\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {config[\"api_key\"]}'\n",
    "    }\n",
    "    data = {\n",
    "        'model': 'gpt-3.5-turbo',\n",
    "        'messages': [\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': 'You are an AI language model trained to perform sentiment analysis. Identify the sentiment of the following text:'\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': text\n",
    "            }\n",
    "        ],\n",
    "        'temperature': 0,\n",
    "        'max_tokens': 32,\n",
    "        'n': 1,\n",
    "    }\n",
    "    url = 'https://api.openai.com/v1/chat/completions'\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    if response.ok:\n",
    "        result = json.loads(response.content)\n",
    "        sentiment = result['choices'][0]['message']['content'].strip()\n",
    "\n",
    "        if 'Sentiment: ' in sentiment:\n",
    "            sentiment_value = sentiment.split('Sentiment: ')[1]\n",
    "            return sentiment_value.lower()\n",
    "        else:\n",
    "            return 'neutral'\n",
    "    else:\n",
    "        response.raise_for_status()\n",
    "\n",
    "def analyze_sentiments(texts):\n",
    "    stats = {'positive': 0, 'negative': 0, 'neutral': 0}\n",
    "    total = len(texts)\n",
    "    for text in texts:\n",
    "        sentiment = get_sentiment(text)\n",
    "        if sentiment in stats:\n",
    "            stats[sentiment] += 1\n",
    "    \n",
    "    stats = {key: (value / total) * 100 for key, value in stats.items()}\n",
    "    return stats\n",
    "\n",
    "def main():\n",
    "    print(\"Welcome to the sentiment analysis tool!\")\n",
    "    texts = []\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"Enter a text for sentiment analysis or type 'done' to finish: \").strip()\n",
    "        if user_input.lower() == 'done' or not user_input:\n",
    "            break\n",
    "        texts.append(user_input)\n",
    "\n",
    "    stats = analyze_sentiments(texts)\n",
    "    print(\"\\nSentiment analysis results:\")\n",
    "    print(f\"Positive: {stats['positive']}%\")\n",
    "    print(f\"Negative: {stats['negative']}%\")\n",
    "    print(f\"Neutral: {stats['neutral']}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LuFdrqVXN_O5",
   "metadata": {
    "cellView": "form",
    "id": "LuFdrqVXN_O5",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "import requests\n",
    "import json\n",
    "import openai\n",
    "\n",
    "# Load the JSON file\n",
    "with open('/content/config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Access the sensitive text examples\n",
    "openai.api_key = config['api_key']\n",
    "\n",
    "def get_sentiment(text):\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {config[\"api_key\"]}'\n",
    "    }\n",
    "    data = {\n",
    "        'model': 'gpt-3.5-turbo',\n",
    "        'messages': [\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': 'You are an AI language model trained to perform sentiment analysis. Identify the sentiment of the following text:'\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': text\n",
    "            }\n",
    "        ],\n",
    "        'temperature': 0,\n",
    "        'max_tokens': 32,\n",
    "        'n': 1,\n",
    "    }\n",
    "    url = 'https://api.openai.com/v1/chat/completions'\n",
    "    print(data['messages'])\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    if response.ok:\n",
    "        result = json.loads(response.content)\n",
    "        sentiment = result['choices'][0]['message']['content'].strip()\n",
    "        print(f\"Model response: {sentiment}\")\n",
    "\n",
    "        # Check if \"Sentiment: \" is in the model response\n",
    "        if 'Sentiment: ' in sentiment:\n",
    "            sentiment_value = sentiment.split('Sentiment: ')[1]\n",
    "\n",
    "            if sentiment_value == 'Positive':\n",
    "                return 1\n",
    "            elif sentiment_value == 'Negative':\n",
    "                return 0\n",
    "            else:\n",
    "                return -1\n",
    "        else:\n",
    "            return -1\n",
    "    else:\n",
    "        response.raise_for_status()\n",
    "\n",
    "def analyze_sentiments(texts):\n",
    "    stats = {'positive': 0, 'negative': 0, 'neutral': 0, 'errors': 0}\n",
    "    total = len(texts)\n",
    "    for text in texts:\n",
    "        sentiment = get_sentiment(text)\n",
    "        if sentiment == 1:\n",
    "            stats['positive'] += 1\n",
    "        elif sentiment == 0:\n",
    "            stats['negative'] += 1\n",
    "        elif sentiment == -1:\n",
    "            stats['neutral'] += 1\n",
    "        else:\n",
    "            stats['errors'] += 1\n",
    "    \n",
    "    stats = {key: (value / total) * 100 for key, value in stats.items()}\n",
    "    return stats\n",
    "\n",
    "def main():\n",
    "    print(\"Welcome to the sentiment analysis tool!\")\n",
    "    texts = []\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"Enter a text for sentiment analysis or type 'done' to finish: \")\n",
    "        if user_input.lower() == 'done':\n",
    "            break\n",
    "        texts.append(user_input)\n",
    "\n",
    "    stats = analyze_sentiments(texts)\n",
    "    print(\"\\nSentiment analysis results:\")\n",
    "    print(f\"Positive: {stats['positive']}%\")\n",
    "    print(f\"Negative: {stats['negative']}%\")\n",
    "    print(f\"Neutral: {stats['neutral']}%\")\n",
    "    print(f\"Errors: {stats['errors']}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HsQDWmrivedI",
   "metadata": {
    "cellView": "form",
    "id": "HsQDWmrivedI",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "import requests\n",
    "import json\n",
    "import openai\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Load the JSON file\n",
    "with open('/content/config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Access the sensitive text examples\n",
    "openai.api_key = config['api_key']\n",
    "\n",
    "# Define function to perform sentiment analysis on a given text\n",
    "def get_sentiment(text):\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {config[\"api_key\"]}'\n",
    "    }\n",
    "    data = {\n",
    "        'model': 'gpt-3.5-turbo',\n",
    "        'messages': [\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': 'You are an AI language model trained to perform sentiment analysis. Identify the sentiment of the following text:'\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': text\n",
    "            }\n",
    "        ],\n",
    "        'temperature': 0,\n",
    "        'max_tokens': 32,\n",
    "        'n': 1,\n",
    "    }\n",
    "    url = 'https://api.openai.com/v1/chat/completions'\n",
    "    print(data['messages'])\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    if response.ok:\n",
    "        result = json.loads(response.content)\n",
    "        sentiment = result['choices'][0]['message']['content'].strip()\n",
    "        print(f\"Model response: {sentiment}\")\n",
    "\n",
    "        # Extract sentiment value after \"Sentiment: \"\n",
    "        sentiment_value = sentiment.split('Sentiment: ')[1]\n",
    "\n",
    "        if sentiment_value == 'Positive':\n",
    "            return 1\n",
    "        elif sentiment_value == 'Negative':\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "    else:\n",
    "        response.raise_for_status()\n",
    "\n",
    "text = \"I really enjoyed the movie, it was great!\"\n",
    "sentiment = get_sentiment(text)\n",
    "print(sentiment)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "29ea450849e14000a7b53667213d0272": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2b213951066e4dedbed7f31c3cbbd86a",
      "placeholder": "​",
      "style": "IPY_MODEL_bf2ae41b9f7242c79cb7d6e82c2342e1",
      "value": " 320/400 [08:08&lt;01:58,  1.48s/it]"
     }
    },
    "2b213951066e4dedbed7f31c3cbbd86a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3338738d2b524a999248e8f03e1800ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9adf6aa633ea4d8c8ee7785664a8505c",
      "max": 400,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6f0a42538b7e4962bf9c90681571dfd4",
      "value": 320
     }
    },
    "5fe34d8fe55b4989887f00e0e2eb38cc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62d7ccf32bce4ffc8d3eab0200491148": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5fe34d8fe55b4989887f00e0e2eb38cc",
      "placeholder": "​",
      "style": "IPY_MODEL_70f254c3bd00492982f290ef7b679b44",
      "value": "Analyzing sentiments:  80%"
     }
    },
    "6f0a42538b7e4962bf9c90681571dfd4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "70f254c3bd00492982f290ef7b679b44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8b643cb65b7744de8007337e13be2e5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_62d7ccf32bce4ffc8d3eab0200491148",
       "IPY_MODEL_3338738d2b524a999248e8f03e1800ad",
       "IPY_MODEL_29ea450849e14000a7b53667213d0272"
      ],
      "layout": "IPY_MODEL_d85408eeb32a44d0828305cc383c6333"
     }
    },
    "9adf6aa633ea4d8c8ee7785664a8505c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf2ae41b9f7242c79cb7d6e82c2342e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d85408eeb32a44d0828305cc383c6333": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
