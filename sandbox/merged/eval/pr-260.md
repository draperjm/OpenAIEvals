## Title: [evals] added multilingual example and support

## Body:

### Eval name

Japanese Oogiri Humor Eval

大喜利や！

### Eval description

- task: asks a model to write a Japanese joke on a subject

- metric: a model-graded eval that evaluates if the text is funny, on a well-prompted scale of 1-5

### What makes this a useful eval?

- first non-English example

- both GPT-3.5 and GPT-4 appear pretty bad at making Japanese jokes, BUT are good at detecting if they are bad jokes ->
  a perfect condition for a model-graded eval

### Eval JSON data

<details>

  <summary>View evals in JSON</summary>

### Eval

  ```jsonl

{"input": "レフ・トルストイで数行の大喜利を書いて。敬語じゃなく関西弁で吉本のM1チャンピョンみたいに書いて。"}

{"input": "グリム兄弟で数行の大喜利を書いて。敬語じゃなく関西弁で吉本のM1チャンピョンみたいに書いて。"}

{"input": "ニコロ・マキャヴェッリで数行の大喜利を書いて。敬語じゃなく関西弁で吉本のM1チャンピョンみたいに書いて。"}

{"input": "トーマス・モアで数行の大喜利を書いて。敬語じゃなく関西弁で吉本のM1チャンピョンみたいに書いて。"}

{"input": "トルストイで数行の大喜利を書いて。敬語じゃなく関西弁で吉本のM1チャンピョンみたいに書いて。"}

  ```

</details>


