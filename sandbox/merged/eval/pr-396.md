Title: Add "which is heavier" eval (Accuracy: 0.03 with 183 tests, trivial for humans)
Body:

### Eval name

which-is-heavier

### Eval description

This evaluation tests the physical reasoning of GPT by asking which of two quantities is heavier, where the quantities
are assigned explicit weights (e.g., "5 kilograms" or "2 pounds") and there is a clear answer (e.g., Q: "Is 5 pounds of
tissue paper heavier than 3 pounds of granite?" A: "Yes"). The catch is that, in each example, the heavier quantity is
always associated with an item that is generally thought of as being light (e.g., feathers, tissue paper, cotton balls)
while the lighter quantity is always associated with an item that is generally thought of as being heavy (for example granite,
cast iron, plutonium). Humans can easily achieve 100% on this task, but they have to cognitively ignore what comprises
the quantities and focus on just their weights.

### What makes this a useful eval?

ChatGPT 3.5 does a decent job of comparing weights (i.e., "Is 5 pounds greater than 3 pounds?" "Yes") as well as
understanding common attributions of "light" and "heavy" to everyday objects like feathers and anvils, but when
combining these two into a single comparison, it appears that the model's bias towards these colloquial attributes makes
it difficult for the model to perform well.

What's interesting is that the errors are not even logically consistent. Consider the following example (taken from the
ChatGPT dialogue UX/UI using GPT-4):



<img width="715" alt="Screen Shot 2023-03-21 at 4 45 44 PM" src="https://user-images.githubusercontent.com/11773823/226827152-f78c0fb4-1136-433c-9d55-65eb6c6c5407.png">



According to GPT-4:

- 15 pounds of hydrogen is heavier than 10 pounds of plutonium

- 20 pounds of hydrogen is **not** heavier than 10 pounds of plutonium

- 30 pounds of hydrogen is heavier than 10 pounds of plutonium

GPT-4 appears to do better than GPT-3.5 in informal testing, but still fails on simple cases and gives inconsistent
results.

This task exposes how adversarial red herrings can potentially hamper performance in quantitative/physical reasoning
tasks such as weight comparison.

The other nice aspect of this task is that one can generate these examples programatically. I can submit a dataset of
thousands rather than just a few hundred if preferred.

### Eval JSON data

<details>

  <summary>View evals in JSON</summary>

### Eval

  ```jsonl

{"input": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Is 2 kilograms of eggshells heavier than 1 kilogram of gold bars? Answer Yes or No"}], "ideal": "Yes"}

{"input": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Is 9 pounds of soap foam heavier than 5 pounds of iron chains? Answer Yes or No"}], "ideal": "Yes"}

{"input": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Is 15 pounds of confetti heavier than 10 pounds of rebar? Answer Yes or No"}], "ideal": "Yes"}

{"input": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Is 1 pound of steel beams heavier than 2 pounds of dust particles? Answer Yes or No"}], "ideal": "No"}

{"input": [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "Is 10 pounds of cast iron heavier than 20 pounds of flakes? Answer Yes or No"}], "ideal": "No"}

  ```

</details>


