The evaluation method described in pull request 393 is called "Escher Sentences". It tests the ability of a model to reason about sentences that look normal but are actually incoherent. The sentences are taken from a research paper's dataset and require a high level of linguistic ability to understand. The evaluation includes questions of varying difficulty, and GPT 3.5 performs at about 20%, worse than random guessing, and 0% on some question sets. The evaluation data is provided in JSON format.