The evaluation method described in pull request 396 is called "which-is-heavier". It tests the physical reasoning of GPT by asking which of two quantities is heavier, where the quantities are assigned explicit weights and there is a clear answer. The evaluation is useful because it exposes how adversarial red herrings can potentially hamper performance in quantitative/physical reasoning tasks such as weight comparison. The evaluation dataset can be generated programmatically and consists of examples where the heavier quantity is always associated with an item that is generally thought of as being light, while the lighter quantity is always associated with an item that is generally thought of as being heavy. The evaluation results show that GPT-3.5 has difficulty performing well on this task due to its bias towards colloquial attributes.