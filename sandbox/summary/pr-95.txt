Pull request 95 describes an evaluation method called "law-true-false" that contains 102 True/False questions based on the American Bar Association's Model Rules of Professional Conduct. The purpose of this evaluation is to understand how well GPT-4 can "understand" legal ethics issues, and to identify areas where the model could benefit from being fine-tuned. The questions are designed to be a baseline for evaluating GPT-4's performance in a legal context, and the results of the evaluation can be used to improve the model's performance. The evaluation data is provided in JSON format, and the questions cover a range of legal ethics issues. The evaluation results show that GPT-3 scored 73% on this set, and the questions will be made harder if GPT-4 performs significantly better. The evaluation also highlights the potential for suggestion bias in GPTs, particularly with regard to numbers in a legal context.