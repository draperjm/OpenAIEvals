The pull request describes an evaluation method for GPT-3.5-Turbo and GPT-4 models on their ability to answer multiple-choice questions related to United States tort law. The evaluation consists of 340 samples, and the accuracy of GPT-4 is reported to be 0.8, while the accuracy of GPT-3.5-Turbo is reported to be 0.58. The evaluation is considered useful for assessing the models' proficiency in a specific domain and identifying areas for improvement. The evaluation is unique because it focuses on a specialized topic, and the accuracy metrics indicate that the models can be improved.